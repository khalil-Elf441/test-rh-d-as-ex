{
  "examId": "ex316-9",
  "title": "OpenShift Virtualization Specialist (EX316) â€” Mock 9: Advanced RBAC, OADP & Cloud-Init",
  "description": "An advanced 15-task exam focusing heavily on RBAC configurations, OADP backup/restore operations, cloud-init customizations, and complex networking scenarios for OpenShift 4.16.",
  "timeLimit": "4h",
  "prerequisites": "# This script should be run once before starting the exam.\n# It simulates the base environment provided to the candidate.\n\necho \"Creating prerequisite resources for Mock 2...\"\n\n# Deploy OADP Operator\necho \"Ensure OADP operator is installed in openshift-adp namespace\"\n# oc create namespace openshift-adp\n# Install OADP operator from OperatorHub\n\n# Create DataProtectionApplication\ncat <<EOF | oc apply -f -\napiVersion: oadp.openshift.io/v1alpha1\nkind: DataProtectionApplication\nmetadata:\n  name: velero\n  namespace: openshift-adp\nspec:\n  configuration:\n    velero:\n      defaultPlugins:\n        - openshift\n        - aws\n    restic:\n      enable: true\n  backupLocations:\n    - velero:\n        provider: aws\n        default: true\n        objectStorage:\n          bucket: oadp-backup-bucket\n          prefix: velero\n        config:\n          region: us-east-1\n          profile: default\n        credential:\n          name: cloud-credentials\n          key: cloud\nEOF\n\n# Ensure default storage class exists\necho \"Verifying default storage class...\"\noc get storageclass\n\n# Ensure RHEL9 and Fedora templates are available\necho \"Verifying VM templates...\"\noc get templates -n openshift | grep -E 'rhel9|fedora'\n\necho \"Prerequisite setup complete. You can now start the exam.\"\n",
  "tasks": [
    {
      "id": "task01",
      "title": "Deploy OpenShift Virtualization with custom configurations",
      "solution": "# 1. Create HyperConverged CR with specific configurations\ncat <<EOF | oc apply -f -\napiVersion: hco.kubevirt.io/v1beta1\nkind: HyperConverged\nmetadata:\n  name: kubevirt-hyperconverged\n  namespace: openshift-cnv\nspec:\n  featureGates:\n    enableCommonBootImageImport: true\n    deployKubeSecondaryDNS: true\n  liveMigrationConfig:\n    completionTimeoutPerGiB: 800\n    parallelMigrationsPerCluster: 5\n    parallelOutboundMigrationsPerNode: 2\n    progressTimeout: 150\n  certConfig:\n    ca:\n      duration: 48h\n      renewBefore: 24h\n  workloadUpdateStrategy:\n    workloadUpdateMethods:\n    - LiveMigrate\n    - Evict\n    batchEvictionSize: 10\n    batchEvictionInterval: 1m\nEOF\n\n# 2. Verify installation\noc get csv -n openshift-cnv\noc get hco -n openshift-cnv\noc get pods -n openshift-cnv\n\n# 3. Wait for all components to be ready\noc wait --for=condition=Available hco kubevirt-hyperconverged -n openshift-cnv --timeout=600s",
      "sections": [
        {
          "title": "OpenShift Virtualization Deployment",
          "notice": "Deploy and configure OpenShift Virtualization operator with custom settings for live migration and certificate management.",
          "subtasks": [
            "Ensure the OpenShift Virtualization operator is installed in the 'openshift-cnv' namespace.",
            "Create a HyperConverged custom resource with the following specifications: enable common boot image import, deploy Kubernetes secondary DNS.",
            "Configure live migration with completion timeout of 800 seconds per GiB, maximum 5 parallel migrations per cluster, and 2 parallel outbound migrations per node.",
            "Set certificate duration to 48 hours with renewal 24 hours before expiration.",
            "Configure workload update strategy to use LiveMigrate and Evict methods with batch eviction size of 10.",
            "Verify all OpenShift Virtualization components are running and the HyperConverged resource shows 'Available' condition."
          ]
        }
      ]
    },
    {
      "id": "task02",
      "title": "Create RBAC policies for multi-tenant VM management",
      "solution": "# 1. Create namespaces for different teams\noc create namespace team-alpha\noc create namespace team-beta\noc create namespace shared-services\n\n# 2. Create users (simulation)\noc create user alice\noc create user bob\noc create user charlie\noc create user admin-user\n\n# 3. Create custom ClusterRole for VM operators\ncat <<EOF | oc apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: vm-operator\nrules:\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"virtualmachines/start\", \"virtualmachines/stop\", \"virtualmachines/restart\"]\n  verbs: [\"update\"]\n- apiGroups: [\"subresources.kubevirt.io\"]\n  resources: [\"virtualmachines/console\", \"virtualmachines/vnc\"]\n  verbs: [\"get\"]\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"delete\"]\nEOF\n\n# 4. Create custom Role for VM viewers (namespace-scoped)\ncat <<EOF | oc apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: vm-viewer\nrules:\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"kubevirt.io\"]\n  resources: [\"virtualmachineinstancemigrations\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"snapshot.kubevirt.io\"]\n  resources: [\"virtualmachinesnapshots\", \"virtualmachinesnapshotcontents\"]\n  verbs: [\"get\", \"list\"]\nEOF\n\n# 5. Create RoleBindings for team-alpha\noc create rolebinding alice-vm-operator --clusterrole=vm-operator --user=alice -n team-alpha\noc create rolebinding bob-vm-viewer --clusterrole=vm-viewer --user=bob -n team-alpha\n\n# 6. Create RoleBindings for team-beta\noc create rolebinding charlie-vm-operator --clusterrole=vm-operator --user=charlie -n team-beta\n\n# 7. Create ClusterRoleBinding for admin across all VM namespaces\noc create clusterrolebinding admin-user-vms --clusterrole=cluster-admin --user=admin-user\n\n# 8. Create a ServiceAccount with limited permissions for automation\noc create sa vm-automation-sa -n shared-services\noc create rolebinding vm-automation-binding --clusterrole=vm-operator --serviceaccount=shared-services:vm-automation-sa -n shared-services\n\n# 9. Verify RBAC configuration\noc auth can-i create virtualmachines --as=alice -n team-alpha\noc auth can-i delete virtualmachines --as=bob -n team-alpha\noc auth can-i list virtualmachines --as=charlie -n team-beta",
      "sections": [
        {
          "title": "Multi-Tenant RBAC Configuration",
          "notice": "Create a comprehensive RBAC structure for managing VMs across multiple teams with different permission levels.",
          "subtasks": [
            "Create three namespaces: 'team-alpha', 'team-beta', and 'shared-services'.",
            "Create four users: 'alice', 'bob', 'charlie', and 'admin-user'.",
            "Create a ClusterRole named 'vm-operator' with full permissions on VMs, VMIs, console/VNC access, and associated resources (PVCs, Services).",
            "Create a ClusterRole named 'vm-viewer' with read-only permissions on VMs, VMIs, migrations, and snapshots.",
            "Grant 'alice' vm-operator permissions in 'team-alpha' namespace only.",
            "Grant 'bob' vm-viewer permissions in 'team-alpha' namespace only.",
            "Grant 'charlie' vm-operator permissions in 'team-beta' namespace only.",
            "Grant 'admin-user' cluster-admin privileges across all namespaces.",
            "Create a ServiceAccount 'vm-automation-sa' in 'shared-services' with vm-operator permissions.",
            "Verify that alice can create VMs in team-alpha, bob cannot delete VMs in team-alpha, and charlie can manage VMs in team-beta."
          ]
        }
      ]
    },
    {
      "id": "task03",
      "title": "Deploy VMs with advanced cloud-init configurations",
      "solution": "# 1. Create ConfigMap with SSH keys\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ssh-keys-cm\n  namespace: team-alpha\ndata:\n  authorized_keys: |\n    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC... alice@workstation\n    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD... admin@jumphost\nEOF\n\n# 2. Create Secret with sensitive data\noc create secret generic app-credentials \\\n  --from-literal=db_password='SecureP@ss123' \\\n  --from-literal=api_token='token-xyz789' \\\n  -n team-alpha\n\n# 3. Create VM with comprehensive cloud-init\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: app-server-01\n  namespace: team-alpha\n  labels:\n    app: web-application\n    tier: backend\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: app-server-01\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 4Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            users:\n              - name: admin\n                groups: wheel\n                sudo: ALL=(ALL) NOPASSWD:ALL\n                lock_passwd: false\n                passwd: \\$6\\$rounds=4096\\$saltsalt\\$hashedpassword\n                ssh_authorized_keys:\n                  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC... alice@workstation\n              - name: appuser\n                groups: users\n                shell: /bin/bash\n            package_update: true\n            package_upgrade: true\n            packages:\n              - httpd\n              - postgresql\n              - python3-pip\n              - git\n              - vim\n            write_files:\n              - path: /etc/yum.repos.d/custom-app.repo\n                permissions: '0644'\n                content: |\n                  [custom-app]\n                  name=Custom Application Repository\n                  baseurl=http://repo.internal.example.com/app-packages\n                  enabled=1\n                  gpgcheck=0\n              - path: /opt/app/config.json\n                permissions: '0600'\n                owner: appuser:appuser\n                content: |\n                  {\n                    \"database\": {\n                      \"host\": \"db-svc.team-alpha.svc.cluster.local\",\n                      \"port\": 5432,\n                      \"name\": \"appdb\"\n                    },\n                    \"cache\": {\n                      \"enabled\": true,\n                      \"ttl\": 3600\n                    }\n                  }\n              - path: /etc/systemd/system/app-service.service\n                permissions: '0644'\n                content: |\n                  [Unit]\n                  Description=Application Service\n                  After=network.target\n                  \n                  [Service]\n                  Type=simple\n                  User=appuser\n                  WorkingDirectory=/opt/app\n                  ExecStart=/usr/bin/python3 /opt/app/server.py\n                  Restart=always\n                  \n                  [Install]\n                  WantedBy=multi-user.target\n            runcmd:\n              - echo \"DB_PASSWORD=SecureP@ss123\" >> /opt/app/.env\n              - echo \"API_TOKEN=token-xyz789\" >> /opt/app/.env\n              - chmod 600 /opt/app/.env\n              - chown appuser:appuser /opt/app/.env\n              - systemctl enable --now httpd\n              - systemctl enable --now postgresql\n              - firewall-cmd --permanent --add-service=http\n              - firewall-cmd --permanent --add-service=https\n              - firewall-cmd --permanent --add-port=8080/tcp\n              - firewall-cmd --reload\n              - setenforce 0\n              - sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config\n              - git clone https://github.com/example/app-repo.git /opt/app/source\n              - pip3 install -r /opt/app/source/requirements.txt\n              - systemctl daemon-reload\n              - systemctl enable app-service.service\n            timezone: America/New_York\n            hostname: app-server-01\n            fqdn: app-server-01.team-alpha.svc.cluster.local\n            manage_etc_hosts: true\n            final_message: \"System boot completed in \\$UPTIME seconds\"\nEOF\n\n# 4. Verify VM is running\noc wait --for=condition=Ready vmi/app-server-01 -n team-alpha --timeout=600s\n\n# 5. Test cloud-init execution\nvirtctl console app-server-01 -n team-alpha\n# Inside VM: sudo cloud-init status --wait\n# Check logs: sudo cat /var/log/cloud-init-output.log",
      "sections": [
        {
          "title": "Advanced Cloud-Init VM Deployment",
          "notice": "Deploy a VM with comprehensive cloud-init configuration including user management, package installation, custom files, and service configuration.",
          "subtasks": [
            "Create a ConfigMap 'ssh-keys-cm' in 'team-alpha' namespace containing SSH public keys.",
            "Create a Secret 'app-credentials' in 'team-alpha' with database password and API token.",
            "Deploy a VM named 'app-server-01' in 'team-alpha' using Fedora 39 container disk.",
            "Configure cloud-init to: create two users ('admin' with sudo access and 'appuser'), install httpd, postgresql, python3-pip, git, and vim packages.",
            "Create three custom files: /etc/yum.repos.d/custom-app.repo for a custom repository, /opt/app/config.json with application configuration, and /etc/systemd/system/app-service.service for a systemd service.",
            "Configure cloud-init to enable httpd and postgresql services, configure firewall rules for HTTP/HTTPS and port 8080.",
            "Set SELinux to permissive mode, clone a git repository, install Python requirements, and enable the custom app-service.",
            "Set timezone to America/New_York and configure hostname and FQDN.",
            "Wait for the VM to reach Ready state and verify cloud-init completed successfully by checking /var/log/cloud-init-output.log."
          ]
        }
      ]
    },
    {
      "id": "task04",
      "title": "Configure NetworkPolicy with complex ingress/egress rules",
      "solution": "# 1. Create NetworkPolicy for app-server\ncat <<EOF | oc apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: app-server-netpol\n  namespace: team-alpha\nspec:\n  podSelector:\n    matchLabels:\n      kubevirt.io/domain: app-server-01\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: team-alpha\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n    - protocol: TCP\n      port: 80\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: shared-services\n      podSelector:\n        matchLabels:\n          app: monitoring\n    ports:\n    - protocol: TCP\n      port: 9090\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: team-alpha\n      podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          app: cache\n    ports:\n    - protocol: TCP\n      port: 6379\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 169.254.169.254/32\n    ports:\n    - protocol: TCP\n      port: 443\n    - protocol: TCP\n      port: 80\nEOF\n\n# 2. Create deny-all default policy\ncat <<EOF | oc apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: team-alpha\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\nEOF\n\n# 3. Verify NetworkPolicy\noc get networkpolicies -n team-alpha\noc describe networkpolicy app-server-netpol -n team-alpha",
      "sections": [
        {
          "title": "Complex NetworkPolicy Configuration",
          "notice": "Create sophisticated NetworkPolicy rules to control traffic to and from the app-server VM.",
          "subtasks": [
            "Create a NetworkPolicy named 'app-server-netpol' in 'team-alpha' namespace targeting the 'app-server-01' VM.",
            "Configure ingress rules to: allow traffic on ports 8080 and 80 from pods labeled 'app=frontend' in 'team-alpha' namespace.",
            "Allow ingress traffic on port 9090 from pods labeled 'app=monitoring' in 'shared-services' namespace.",
            "Configure egress rules to: allow traffic to pods labeled 'app=database' on port 5432 in 'team-alpha' namespace.",
            "Allow egress to pods labeled 'app=cache' on port 6379 in any namespace.",
            "Allow egress to kube-dns on UDP port 53 in 'kube-system' namespace.",
            "Allow egress to all external IPs on ports 80 and 443, except the metadata service IP (169.254.169.254).",
            "Create a 'default-deny-all' NetworkPolicy in 'team-alpha' to block all traffic by default.",
            "Verify both NetworkPolicies are created and describe their rules."
          ]
        }
      ]
    },
    {
      "id": "task05",
      "title": "Implement OADP backup strategy with schedules and hooks",
      "solution": "# 1. Create backup schedule for daily backups\ncat <<EOF | oc apply -f -\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: daily-vm-backup\n  namespace: openshift-adp\nspec:\n  schedule: \"0 2 * * *\"\n  template:\n    includedNamespaces:\n    - team-alpha\n    - team-beta\n    excludedResources:\n    - events\n    - events.events.k8s.io\n    labelSelector:\n      matchLabels:\n        backup: \"true\"\n    snapshotVolumes: true\n    ttl: 720h0m0s\n    hooks:\n      resources:\n      - name: vm-quiesce-hook\n        includedNamespaces:\n        - team-alpha\n        labelSelector:\n          matchLabels:\n            app: database\n        pre:\n        - exec:\n            container: compute\n            command:\n            - /bin/bash\n            - -c\n            - \"sync; echo 'Flushing database' > /tmp/backup.log\"\n            timeout: 30s\n        post:\n        - exec:\n            container: compute\n            command:\n            - /bin/bash\n            - -c\n            - \"echo 'Backup completed' >> /tmp/backup.log\"\nEOF\n\n# 2. Create on-demand backup with specific resources\ncat <<EOF | oc apply -f -\napiVersion: velero.io/v1\nkind: Backup\nmetadata:\n  name: app-server-backup-001\n  namespace: openshift-adp\nspec:\n  includedNamespaces:\n  - team-alpha\n  includedResources:\n  - virtualmachines\n  - virtualmachineinstances\n  - persistentvolumeclaims\n  - persistentvolumes\n  - configmaps\n  - secrets\n  - services\n  labelSelector:\n    matchLabels:\n      app: web-application\n  snapshotVolumes: true\n  defaultVolumesToRestic: false\n  ttl: 168h0m0s\n  storageLocation: default\nEOF\n\n# 3. Wait for backup completion\noc wait --for=jsonpath='{.status.phase}'=Completed backup/app-server-backup-001 -n openshift-adp --timeout=600s\n\n# 4. Verify backup\noc get backup app-server-backup-001 -n openshift-adp -o jsonpath='{.status}'\noc describe backup app-server-backup-001 -n openshift-adp",
      "sections": [
        {
          "title": "OADP Backup Strategy Implementation",
          "notice": "Configure comprehensive backup strategies using OADP with schedules, hooks, and selective resource inclusion.",
          "subtasks": [
            "Create a Schedule resource named 'daily-vm-backup' in 'openshift-adp' namespace that runs daily at 2 AM.",
            "Configure the schedule to backup 'team-alpha' and 'team-beta' namespaces, excluding events.",
            "Use label selector to only backup resources with label 'backup=true'.",
            "Enable volume snapshots and set TTL to 720 hours (30 days).",
            "Configure pre-backup hooks for VMs labeled 'app=database' to execute a sync command with 30-second timeout.",
            "Configure post-backup hooks to log backup completion.",
            "Create an on-demand Backup named 'app-server-backup-001' for 'team-alpha' namespace.",
            "Include only VMs, VMIs, PVCs, PVs, ConfigMaps, Secrets, and Services in the backup.",
            "Use label selector to backup only resources labeled 'app=web-application'.",
            "Set TTL to 168 hours (7 days) and use default storage location.",
            "Wait for backup completion and verify the backup status shows 'Completed'."
          ]
        }
      ]
    },
    {
      "id": "task06",
      "title": "Perform OADP restore with transformations",
      "solution": "# 1. Create restore with namespace mapping\ncat <<EOF | oc apply -f -\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: app-server-restore-001\n  namespace: openshift-adp\nspec:\n  backupName: app-server-backup-001\n  includedNamespaces:\n  - team-alpha\n  namespaceMapping:\n    team-alpha: team-alpha-restored\n  restorePVs: true\n  existingResourcePolicy: update\n  preserveNodePorts: false\n  includeClusterResources: false\n  labelSelector:\n    matchLabels:\n      app: web-application\n  hooks:\n    resources:\n    - name: post-restore-config\n      includedNamespaces:\n      - team-alpha-restored\n      postHooks:\n      - exec:\n          container: compute\n          command:\n          - /bin/bash\n          - -c\n          - \"echo 'Restore completed at $(date)' > /var/log/restore.log\"\n          waitTimeout: 5m\n          execTimeout: 1m\nEOF\n\n# 2. Monitor restore progress\noc get restore app-server-restore-001 -n openshift-adp -w\n\n# 3. Wait for restore completion\noc wait --for=jsonpath='{.status.phase}'=Completed restore/app-server-restore-001 -n openshift-adp --timeout=600s\n\n# 4. Verify restored resources\noc get vms -n team-alpha-restored\noc get vmis -n team-alpha-restored\noc get pvc -n team-alpha-restored\n\n# 5. Check restore logs\noc logs -n openshift-adp deployment/velero --tail=100 | grep app-server-restore-001",
      "sections": [
        {
          "title": "OADP Restore with Namespace Mapping",
          "notice": "Restore the previously backed up application to a new namespace with transformations and post-restore hooks.",
          "subtasks": [
            "Create a Restore resource named 'app-server-restore-001' in 'openshift-adp' namespace.",
            "Restore from backup 'app-server-backup-001'.",
            "Map the source namespace 'team-alpha' to destination namespace 'team-alpha-restored'.",
            "Enable PV restoration and set existing resource policy to 'update'.",
            "Disable node port preservation and cluster resource inclusion.",
            "Use label selector to restore only resources labeled 'app=web-application'.",
            "Configure post-restore hooks to execute a command that logs the restore completion time to /var/log/restore.log.",
            "Set wait timeout to 5 minutes and exec timeout to 1 minute for hooks.",
            "Monitor the restore progress and wait until status shows 'Completed'.",
            "Verify restored VMs, VMIs, and PVCs exist in 'team-alpha-restored' namespace.",
            "Check velero logs for restore operation details."
          ]
        }
      ]
    },
    {
      "id": "task07",
      "title": "Configure multus networking with bridge and macvlan networks",
      "solution": "# 1. Create linux-bridge NetworkAttachmentDefinition\ncat <<EOF | oc apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: bridge-network-vlan100\n  namespace: team-beta\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"bridge-network-vlan100\",\n      \"type\": \"cnv-bridge\",\n      \"bridge\": \"br1\",\n      \"vlan\": 100,\n      \"ipam\": {\n        \"type\": \"static\"\n      },\n      \"macspoofchk\": true,\n      \"preserveDefaultVlan\": false\n    }\nEOF\n\n# 2. Create macvlan NetworkAttachmentDefinition\ncat <<EOF | oc apply -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-network\n  namespace: team-beta\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"name\": \"macvlan-network\",\n      \"type\": \"macvlan\",\n      \"master\": \"eth0\",\n      \"mode\": \"bridge\",\n      \"ipam\": {\n        \"type\": \"whereabouts\",\n        \"range\": \"192.168.200.0/24\",\n        \"gateway\": \"192.168.200.1\"\n      }\n    }\nEOF\n\n# 3. Create multi-homed VM with multiple interfaces\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: network-gateway-vm\n  namespace: team-beta\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: network-gateway-vm\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 2Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n          - name: bridge-net\n            bridge: {}\n          - name: macvlan-net\n            bridge: {}\n      networks:\n      - name: default\n        pod: {}\n      - name: bridge-net\n        multus:\n          networkName: bridge-network-vlan100\n      - name: macvlan-net\n        multus:\n          networkName: macvlan-network\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          networkData: |\n            version: 2\n            ethernets:\n              eth0:\n                dhcp4: true\n              eth1:\n                addresses:\n                - 10.100.0.10/24\n              eth2:\n                dhcp4: true\n          userData: |\n            #cloud-config\n            runcmd:\n              - sysctl -w net.ipv4.ip_forward=1\n              - echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf\nEOF\n\n# 4. Verify VM networking\noc wait --for=condition=Ready vmi/network-gateway-vm -n team-beta --timeout=300s\nvirtctl console network-gateway-vm -n team-beta\n# Inside VM: ip addr show",
      "sections": [
        {
          "title": "Advanced Multus Multi-Homed Networking",
          "notice": "Create NetworkAttachmentDefinitions using different CNI plugins and deploy a multi-homed VM with three network interfaces.",
          "subtasks": [
            "Create a NetworkAttachmentDefinition named 'bridge-network-vlan100' in 'team-beta' namespace using cnv-bridge type with VLAN 100, bridge 'br1', and MAC spoofing check enabled.",
            "Create a NetworkAttachmentDefinition named 'macvlan-network' in 'team-beta' namespace using macvlan type with whereabouts IPAM, range 192.168.200.0/24, and gateway 192.168.200.1.",
            "Deploy a VM named 'network-gateway-vm' in 'team-beta' namespace with three network interfaces: default pod network (masquerade), bridge-network-vlan100 (bridge), and macvlan-network (bridge).",
            "Configure cloud-init networkData to set eth1 with static IP 10.100.0.10/24 and enable IP forwarding in the VM.",
            "Verify the VM has three network interfaces and can route traffic between networks."
          ]
        }
      ]
    },
    {
      "id": "task08",
      "title": "Configure VM snapshots with retention policy",
      "solution": "# 1. Create VirtualMachineSnapshot\ncat <<EOF | oc apply -f -\napiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineSnapshot\nmetadata:\n  name: app-server-snap-pre-upgrade\n  namespace: team-alpha\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: app-server-01\n  failureDeadline: 5m\nEOF\n\n# 2. Wait for snapshot to be ready\noc wait --for=jsonpath='{.status.readyToUse}'=true vmsnap/app-server-snap-pre-upgrade -n team-alpha --timeout=300s\n\n# 3. Verify snapshot content\noc get vmsnapshot app-server-snap-pre-upgrade -n team-alpha -o yaml\noc get vmsnapshotcontent -n team-alpha\n\n# 4. Make changes to the VM (simulate upgrade)\nvirtctl stop app-server-01 -n team-alpha\noc patch vm app-server-01 -n team-alpha --type=merge -p '{\"spec\":{\"template\":{\"spec\":{\"domain\":{\"memory\":{\"guest\":\"6Gi\"}}}}}}'\nvirtctl start app-server-01 -n team-alpha\n\n# 5. Create restore point\ncat <<EOF | oc apply -f -\napiVersion: snapshot.kubevirt.io/v1alpha1\nkind: VirtualMachineRestore\nmetadata:\n  name: app-server-restore-pre-upgrade\n  namespace: team-alpha\nspec:\n  target:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: app-server-01\n  virtualMachineSnapshotName: app-server-snap-pre-upgrade\nEOF\n\n# 6. Monitor restore\noc wait --for=jsonpath='{.status.complete}'=true vmrestore/app-server-restore-pre-upgrade -n team-alpha --timeout=300s\n\n# 7. Verify VM is restored\noc get vm app-server-01 -n team-alpha -o jsonpath='{.spec.template.spec.domain.memory.guest}'",
      "sections": [
        {
          "title": "VM Snapshot and Restore Operations",
          "notice": "Create snapshots of VMs before upgrades and restore from snapshots when needed.",
          "subtasks": [
            "Create a VirtualMachineSnapshot named 'app-server-snap-pre-upgrade' for VM 'app-server-01' in 'team-alpha' namespace.",
            "Set failure deadline to 5 minutes for the snapshot operation.",
            "Wait for snapshot to reach 'readyToUse' status and verify VirtualMachineSnapshotContent is created.",
            "Stop the VM and modify its memory allocation from 4Gi to 6Gi to simulate an upgrade.",
            "Start the VM again and verify the memory change is applied.",
            "Create a VirtualMachineRestore named 'app-server-restore-pre-upgrade' to restore the VM to the snapshot state.",
            "Monitor the restore operation until completion.",
            "Verify the VM memory has been restored to the original 4Gi value."
          ]
        }
      ]
    },
    {
      "id": "task09",
      "title": "Configure VM affinity and anti-affinity rules",
      "solution": "# 1. Label nodes for affinity\nNODE1=$(oc get nodes -l node-role.kubernetes.io/worker= -o jsonpath='{.items[0].metadata.name}')\nNODE2=$(oc get nodes -l node-role.kubernetes.io/worker= -o jsonpath='{.items[1].metadata.name}')\n\noc label node $NODE1 workload-type=compute-intensive\noc label node $NODE2 workload-type=storage-intensive\noc label node $NODE1 availability-zone=az1\noc label node $NODE2 availability-zone=az2\n\n# 2. Create VM with node affinity\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: compute-vm-01\n  namespace: team-beta\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: compute-vm-01\n        app-tier: compute\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: workload-type\n                operator: In\n                values:\n                - compute-intensive\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: availability-zone\n                operator: In\n                values:\n                - az1\n      domain:\n        cpu:\n          cores: 4\n        memory:\n          guest: 8Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\nEOF\n\n# 3. Create VM with pod anti-affinity\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: compute-vm-02\n  namespace: team-beta\n  labels:\n    app-tier: compute\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: compute-vm-02\n        app-tier: compute\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app-tier\n                operator: In\n                values:\n                - compute\n            topologyKey: kubernetes.io/hostname\n      domain:\n        cpu:\n          cores: 4\n        memory:\n          guest: 8Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\nEOF\n\n# 4. Verify placement\noc get vmi -n team-beta -o wide\noc get vmi compute-vm-01 -n team-beta -o jsonpath='{.status.nodeName}'\noc get vmi compute-vm-02 -n team-beta -o jsonpath='{.status.nodeName}'",
      "sections": [
        {
          "title": "VM Placement with Affinity Rules",
          "notice": "Configure node affinity and pod anti-affinity to control VM placement across cluster nodes.",
          "subtasks": [
            "Label two worker nodes: first node with 'workload-type=compute-intensive' and 'availability-zone=az1', second node with 'workload-type=storage-intensive' and 'availability-zone=az2'.",
            "Create a VM named 'compute-vm-01' in 'team-beta' namespace with 4 CPU cores and 8Gi memory.",
            "Configure node affinity to require scheduling on nodes with 'workload-type=compute-intensive' label.",
            "Add preferred affinity (weight 100) for nodes in 'availability-zone=az1'.",
            "Create a second VM named 'compute-vm-02' in 'team-beta' namespace with same resources.",
            "Configure pod anti-affinity to prevent scheduling on the same node as other VMs with label 'app-tier=compute'.",
            "Verify 'compute-vm-01' is scheduled on the compute-intensive node.",
            "Verify 'compute-vm-02' is scheduled on a different node than 'compute-vm-01'."
          ]
        }
      ]
    },
    {
      "id": "task10",
      "title": "Configure VM health probes and watchdog",
      "solution": "# 1. Create VM with comprehensive health probes\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: monitored-app-vm\n  namespace: shared-services\nspec:\n  running: true\n  runStrategy: Always\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: monitored-app-vm\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 4Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n          watchdog:\n            name: watchdog0\n            i6300esb:\n              action: reset\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            packages:\n              - httpd\n            runcmd:\n              - systemctl enable --now httpd\n              - echo '<html><body>Health Check OK</body></html>' > /var/www/html/health\n      readinessProbe:\n        httpGet:\n          port: 80\n          path: /health\n        initialDelaySeconds: 120\n        periodSeconds: 10\n        timeoutSeconds: 5\n        failureThreshold: 3\n        successThreshold: 1\n      livenessProbe:\n        httpGet:\n          port: 80\n          path: /health\n        initialDelaySeconds: 180\n        periodSeconds: 20\n        timeoutSeconds: 10\n        failureThreshold: 3\nEOF\n\n# 2. Wait for VM to be ready\noc wait --for=condition=Ready vmi/monitored-app-vm -n shared-services --timeout=600s\n\n# 3. Verify probes are working\noc get vmi monitored-app-vm -n shared-services -o jsonpath='{.status.conditions}'\n\n# 4. Test watchdog (inside VM)\n# virtctl console monitored-app-vm -n shared-services\n# systemctl status watchdog\n# Install watchdog if needed: dnf install -y watchdog\n# Configure: echo 'watchdog-device = /dev/watchdog0' > /etc/watchdog.conf\n# systemctl enable --now watchdog",
      "sections": [
        {
          "title": "Health Monitoring and Watchdog Configuration",
          "notice": "Configure comprehensive health monitoring for VMs including readiness probes, liveness probes, and watchdog devices.",
          "subtasks": [
            "Create a VM named 'monitored-app-vm' in 'shared-services' namespace with 2 CPU cores and 4Gi memory.",
            "Set run strategy to 'Always' to ensure the VM restarts on failure.",
            "Configure a watchdog device (i6300esb) with reset action.",
            "Use cloud-init to install httpd and create a /health endpoint.",
            "Configure readiness probe: HTTP GET on port 80, path /health, initial delay 120s, period 10s, timeout 5s, failure threshold 3.",
            "Configure liveness probe: HTTP GET on port 80, path /health, initial delay 180s, period 20s, timeout 10s, failure threshold 3.",
            "Wait for VM to reach Ready state.",
            "Verify probe status in VMI conditions.",
            "Access the VM console and verify watchdog service is available and can be configured."
          ]
        }
      ]
    },
    {
      "id": "task11",
      "title": "Configure VM with hotplug volumes",
      "solution": "# 1. Create PVCs for hotplug\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume-01\n  namespace: team-beta\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume-02\n  namespace: team-beta\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 15Gi\nEOF\n\n# 2. Create VM with hotplug feature enabled\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: storage-vm\n  namespace: team-beta\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: storage-vm\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 4Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            packages:\n              - lvm2\nEOF\n\n# 3. Wait for VM to be running\noc wait --for=condition=Ready vmi/storage-vm -n team-beta --timeout=300s\n\n# 4. Add volume hotplug\nvirtctl addvolume storage-vm --volume-name=data-volume-01 --disk-name=hotplug-disk-01 --persist -n team-beta\n\n# 5. Wait and verify hotplug\nsleep 30\noc get vmi storage-vm -n team-beta -o jsonpath='{.spec.volumes}' | grep data-volume-01\n\n# 6. Add second hotplug volume\nvirtctl addvolume storage-vm --volume-name=data-volume-02 --disk-name=hotplug-disk-02 --persist -n team-beta\n\n# 7. Verify inside VM\nvirtctl console storage-vm -n team-beta\n# lsblk (should show new disks)\n\n# 8. Remove hotplug volume (optional test)\n# virtctl removevolume storage-vm --volume-name=data-volume-01 -n team-beta",
      "sections": [
        {
          "title": "Hotplug Volume Management",
          "notice": "Configure and manage hotplug volumes on running VMs without requiring restarts.",
          "subtasks": [
            "Create two PVCs in 'team-beta' namespace: 'data-volume-01' (10Gi) and 'data-volume-02' (15Gi).",
            "Deploy a VM named 'storage-vm' in 'team-beta' namespace with 2 CPU cores and 4Gi memory.",
            "Use cloud-init to install lvm2 package for volume management.",
            "Wait for the VM to reach Ready state.",
            "Use virtctl to hotplug 'data-volume-01' to the running VM with persistent configuration.",
            "Verify the volume appears in the VMI specification.",
            "Hotplug the second volume 'data-volume-02' to the same VM.",
            "Access the VM console and verify both new disks are visible using lsblk command.",
            "Ensure hotplugged volumes persist across VM restarts."
          ]
        }
      ]
    },
    {
      "id": "task12",
      "title": "Configure NodePort service and custom Route for VM",
      "solution": "# 1. Create VM for web service\ncat <<EOF | oc apply -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: public-web-vm\n  namespace: shared-services\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: public-web-vm\n        service: web\n    spec:\n      domain:\n        cpu:\n          cores: 2\n        memory:\n          guest: 2Gi\n        devices:\n          disks:\n          - name: containerdisk\n            disk:\n              bus: virtio\n          - name: cloudinitdisk\n            disk:\n              bus: virtio\n          interfaces:\n          - name: default\n            masquerade: {}\n      networks:\n      - name: default\n        pod: {}\n      volumes:\n      - name: containerdisk\n        containerDisk:\n          image: quay.io/containerdisks/fedora:39\n      - name: cloudinitdisk\n        cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            packages:\n              - httpd\n              - mod_ssl\n            write_files:\n              - path: /var/www/html/index.html\n                content: |\n                  <html>\n                  <head><title>Public Web VM</title></head>\n                  <body><h1>Welcome to Public Web VM</h1></body>\n                  </html>\n            runcmd:\n              - systemctl enable --now httpd\n              - firewall-cmd --permanent --add-service=http\n              - firewall-cmd --permanent --add-service=https\n              - firewall-cmd --reload\nEOF\n\n# 2. Wait for VM\noc wait --for=condition=Ready vmi/public-web-vm -n shared-services --timeout=300s\n\n# 3. Create ClusterIP service\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: public-web-clusterip\n  namespace: shared-services\nspec:\n  selector:\n    kubevirt.io/domain: public-web-vm\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  - name: https\n    protocol: TCP\n    port: 443\n    targetPort: 443\n  type: ClusterIP\nEOF\n\n# 4. Create NodePort service\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: public-web-nodeport\n  namespace: shared-services\nspec:\n  selector:\n    kubevirt.io/domain: public-web-vm\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 80\n    nodePort: 30080\n  - name: https\n    protocol: TCP\n    port: 443\n    targetPort: 443\n    nodePort: 30443\n  type: NodePort\nEOF\n\n# 5. Create Route with TLS\ncat <<EOF | oc apply -f -\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: public-web-route\n  namespace: shared-services\nspec:\n  host: public-web.apps.cluster.example.com\n  to:\n    kind: Service\n    name: public-web-clusterip\n    weight: 100\n  port:\n    targetPort: http\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\nEOF\n\n# 6. Verify services and route\noc get svc -n shared-services\noc get route public-web-route -n shared-services\n\n# 7. Test access\nNODE_IP=$(oc get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\ncurl -I http://$NODE_IP:30080",
      "sections": [
        {
          "title": "Service and Route Configuration for External Access",
          "notice": "Configure multiple service types and custom routes to expose a VM web application externally.",
          "subtasks": [
            "Create a VM named 'public-web-vm' in 'shared-services' namespace with httpd installed via cloud-init.",
            "Configure cloud-init to create a custom index.html page and enable httpd service.",
            "Configure firewall rules to allow HTTP and HTTPS traffic.",
            "Create a ClusterIP service named 'public-web-clusterip' exposing ports 80 and 443.",
            "Create a NodePort service named 'public-web-nodeport' with nodePort 30080 for HTTP and 30443 for HTTPS.",
            "Create a Route named 'public-web-route' with custom hostname 'public-web.apps.cluster.example.com'.",
            "Configure the Route with edge TLS termination and redirect insecure traffic to HTTPS.",
            "Verify services are created and Route is accessible.",
            "Test NodePort access using node IP and port 30080."
          ]
        }
      ]
    },
    {
      "id": "task13",
      "title": "Perform live migration with custom migration policies",
      "solution": "# 1. Create MigrationPolicy\ncat <<EOF | oc apply -f -\napiVersion: migrations.kubevirt.io/v1alpha1\nkind: MigrationPolicy\nmetadata:\n  name: custom-migration-policy\nspec:\n  selectors:\n    namespaceSelector:\n      matchLabels:\n        migration-policy: custom\n    virtualMachineInstanceSelector:\n      matchLabels:\n        migration-priority: high\n  allowAutoConverge: true\n  allowPostCopy: false\n  completionTimeoutPerGiB: 800\n  bandwidthPerMigration: 256Mi\nEOF\n\n# 2. Label namespace for migration policy\noc label namespace team-alpha migration-policy=custom\n\n# 3. Update VM with migration label\noc patch vm app-server-01 -n team-alpha --type=merge -p '{\"metadata\":{\"labels\":{\"migration-priority\":\"high\"}}}'\n\n# 4. Verify VM is running\noc get vmi app-server-01 -n team-alpha -o jsonpath='{.status.nodeName}'\nORIGINAL_NODE=$(oc get vmi app-server-01 -n team-alpha -o jsonpath='{.status.nodeName}')\n\n# 5. Initiate migration\nvirtctl migrate app-server-01 -n team-alpha\n\n# 6. Monitor migration\noc get vmim -n team-alpha -w\n\n# 7. Wait for migration completion\noc wait --for=jsonpath='{.status.phase}'=Succeeded vmim -l kubevirt.io/vmi-name=app-server-01 -n team-alpha --timeout=600s\n\n# 8. Verify new node\nNEW_NODE=$(oc get vmi app-server-01 -n team-alpha -o jsonpath='{.status.nodeName}')\necho \"VM migrated from $ORIGINAL_NODE to $NEW_NODE\"\n\n# 9. Check migration metrics\noc get vmim -n team-alpha -o yaml",
      "sections": [
        {
          "title": "Live Migration with Custom Policies",
          "notice": "Configure and execute live migrations using custom MigrationPolicy resources with specific parameters.",
          "subtasks": [
            "Create a MigrationPolicy named 'custom-migration-policy' with namespace and VM selectors.",
            "Configure the policy to: enable auto-converge, disable post-copy, set completion timeout to 800 seconds per GiB, and limit bandwidth to 256Mi per migration.",
            "Apply the policy to namespaces labeled 'migration-policy=custom' and VMs labeled 'migration-priority=high'.",
            "Label 'team-alpha' namespace with 'migration-policy=custom'.",
            "Update 'app-server-01' VM with label 'migration-priority=high'.",
            "Record the current node where 'app-server-01' is running.",
            "Initiate a live migration using virtctl migrate command.",
            "Monitor the VirtualMachineInstanceMigration (vmim) resource and watch the migration progress.",
            "Wait for migration to complete successfully (status: Succeeded).",
            "Verify the VM is now running on a different node and check migration metrics."
          ]
        }
      ]
    },
    {
      "id": "task14",
      "title": "Configure node maintenance with VM evacuation",
      "solution": "# 1. Select a worker node with VMs\nNODE_TO_MAINTAIN=$(oc get vmi -A -o jsonpath='{.items[0].status.nodeName}')\necho \"Selected node for maintenance: $NODE_TO_MAINTAIN\"\n\n# 2. Check VMs on the node\noc get vmi -A -o wide | grep $NODE_TO_MAINTAIN\n\n# 3. Taint the node\noc adm taint node $NODE_TO_MAINTAIN maintenance=scheduled:NoSchedule\n\n# 4. Mark node unschedulable\noc adm cordon $NODE_TO_MAINTAIN\n\n# 5. Verify node status\noc get node $NODE_TO_MAINTAIN\n\n# 6. Drain the node (this will live migrate VMs)\noc adm drain $NODE_TO_MAINTAIN --ignore-daemonsets --delete-emptydir-data --force --grace-period=300\n\n# 7. Monitor VM migrations\noc get vmim -A -w\n\n# 8. Wait for all VMs to be evacuated\nwhile [ $(oc get vmi -A -o json | jq -r \".items[] | select(.status.nodeName==\\\"$NODE_TO_MAINTAIN\\\") | .metadata.name\" | wc -l) -gt 0 ]; do\n  echo \"Waiting for VMs to evacuate...\"\n  sleep 10\ndone\n\n# 9. Verify no VMs remain on the node\noc get vmi -A -o wide | grep $NODE_TO_MAINTAIN\n\n# 10. Perform maintenance (simulation)\necho \"Node $NODE_TO_MAINTAIN is ready for maintenance\"\n\n# 11. After maintenance, uncordon the node\n# oc adm uncordon $NODE_TO_MAINTAIN\n# oc adm taint node $NODE_TO_MAINTAIN maintenance:NoSchedule-",
      "sections": [
        {
          "title": "Node Maintenance with VM Evacuation",
          "notice": "Safely prepare a node for maintenance by evacuating all VMs through live migration.",
          "subtasks": [
            "Identify a worker node that is currently running at least one VM.",
            "List all VMs running on the selected node to verify workload.",
            "Taint the node with 'maintenance=scheduled:NoSchedule' to prevent new pods from scheduling.",
            "Mark the node as unschedulable using 'oc adm cordon' command.",
            "Verify the node status shows 'SchedulingDisabled'.",
            "Drain the node using 'oc adm drain' with flags: ignore-daemonsets, delete-emptydir-data, force, and grace-period of 300 seconds.",
            "Monitor all VirtualMachineInstanceMigration resources to track evacuation progress.",
            "Wait until all VMs have been successfully migrated off the node.",
            "Verify no VMIs remain on the node after evacuation.",
            "Confirm the node is ready for maintenance activities."
          ]
        }
      ]
    },
    {
      "id": "task15",
      "title": "Clone VM and customize with cloud-init override",
      "solution": "# 1. Ensure source VM is stopped for consistent clone\nvirtctl stop app-server-01 -n team-alpha\noc wait --for=condition=Ready=false vmi/app-server-01 -n team-alpha --timeout=120s\n\n# 2. Create VM clone\ncat <<EOF | oc apply -f -\napiVersion: clone.kubevirt.io/v1alpha1\nkind: VirtualMachineClone\nmetadata:\n  name: app-server-clone-dev\n  namespace: team-alpha\nspec:\n  source:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: app-server-01\n  target:\n    apiGroup: kubevirt.io\n    kind: VirtualMachine\n    name: app-server-dev\n  labelFilters:\n    - \"*\"\n  annotationFilters:\n    - \"*\"\n  templateFilters:\n    - \"*\"\nEOF\n\n# 3. Wait for clone to complete\noc wait --for=jsonpath='{.status.phase}'=Succeeded vmclone/app-server-clone-dev -n team-alpha --timeout=600s\n\n# 4. Verify cloned VM exists\noc get vm app-server-dev -n team-alpha\n\n# 5. Customize cloned VM with new cloud-init\ncat <<EOF | oc apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-server-dev-cloudinit\n  namespace: team-alpha\ntype: Opaque\nstringData:\n  userdata: |\n    #cloud-config\n    users:\n      - name: developer\n        groups: wheel\n        sudo: ALL=(ALL) NOPASSWD:ALL\n        ssh_authorized_keys:\n          - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD... developer@workstation\n    hostname: app-server-dev\n    fqdn: app-server-dev.team-alpha.svc.cluster.local\n    write_files:\n      - path: /etc/environment\n        content: |\n          ENVIRONMENT=development\n          DEBUG=true\n          LOG_LEVEL=debug\n      - path: /opt/app/dev-config.json\n        permissions: '0644'\n        content: |\n          {\n            \"environment\": \"development\",\n            \"debug_mode\": true,\n            \"database\": {\n              \"host\": \"dev-db-svc.team-alpha.svc.cluster.local\",\n              \"port\": 5432\n            }\n          }\n    runcmd:\n      - echo \"Development environment initialized\" > /var/log/dev-init.log\n      - sed -i 's/ServerName production/ServerName development/g' /etc/httpd/conf/httpd.conf\n      - systemctl restart httpd\nEOF\n\n# 6. Patch cloned VM to use new cloud-init\noc patch vm app-server-dev -n team-alpha --type=json -p='[\n  {\n    \"op\": \"replace\",\n    \"path\": \"/spec/template/spec/volumes/1\",\n    \"value\": {\n      \"name\": \"cloudinitdisk\",\n      \"cloudInitNoCloud\": {\n        \"secretRef\": {\n          \"name\": \"app-server-dev-cloudinit\"\n        }\n      }\n    }\n  }\n]'\n\n# 7. Update VM labels and annotations\noc label vm app-server-dev -n team-alpha environment=development tier=backend-dev --overwrite\noc annotate vm app-server-dev -n team-alpha description=\"Development clone of app-server-01\" --overwrite\n\n# 8. Start the cloned VM\nvirtctl start app-server-dev -n team-alpha\n\n# 9. Wait for VM to be ready\noc wait --for=condition=Ready vmi/app-server-dev -n team-alpha --timeout=600s\n\n# 10. Verify customization\nvirtctl console app-server-dev -n team-alpha\n# Inside VM: cat /etc/environment\n# cat /opt/app/dev-config.json\n# hostname\n\n# 11. Restart original VM\nvirtctl start app-server-01 -n team-alpha",
      "sections": [
        {
          "title": "VM Cloning with Cloud-Init Customization",
          "notice": "Clone an existing VM and customize it with different cloud-init configuration for a development environment.",
          "subtasks": [
            "Stop the source VM 'app-server-01' in 'team-alpha' namespace to ensure a consistent clone.",
            "Create a VirtualMachineClone resource named 'app-server-clone-dev' to clone 'app-server-01' to 'app-server-dev'.",
            "Configure the clone to copy all labels, annotations, and template filters.",
            "Wait for the clone operation to complete successfully (status: Succeeded).",
            "Verify the cloned VM 'app-server-dev' exists in the namespace.",
            "Create a Secret 'app-server-dev-cloudinit' with customized cloud-init data for development environment.",
            "Configure cloud-init to: add a 'developer' user with sudo access, set hostname to 'app-server-dev', create environment configuration files with DEBUG=true and development settings.",
            "Patch the cloned VM to replace the cloud-init volume with the new Secret reference.",
            "Add labels 'environment=development' and 'tier=backend-dev' to the cloned VM.",
            "Add annotation describing the VM as a development clone.",
            "Start the cloned VM and wait for it to reach Ready state.",
            "Verify customizations by checking /etc/environment, hostname, and config files inside the VM.",
            "Restart the original VM 'app-server-01'."
          ]
        }
      ]
    },
    {
      "id": "task16",
      "title": "Create a VM from a qcow2 URL",
      "solution": "# 1. Create DataVolume\noc create -f - -n default <<EOF\napiVersion: cdi.kubevirt.io/v1beta1\nkind: DataVolume\nmetadata:\n  name: fedora-dv\nspec:\n  source:\n    http:\n      url: https://download.fedoraproject.org/pub/fedora/linux/releases/36/Cloud/x86_64/images/Fedora-Cloud-Base-36-1.5.x86_64.qcow2\n  pvc:\n    accessModes:\n    - ReadWriteOnce\n    resources:\n      requests:\n        storage: 10Gi\nEOF\n\n# 2. Create VM\noc create -f - -n default <<EOF\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: vm-from-qcow2\nspec:\n  running: true\n  template:\n    spec:\n      domain:\n        devices:\n          disks:\n          - name: rootdisk\n            disk:\n              bus: virtio\n        resources:\n          requests:\n            memory: 2Gi\n      volumes:\n      - name: rootdisk\n        dataVolume:\n          name: fedora-dv\nEOF",
      "sections": [
        {
          "title": "Create VM from qcow2 URL",
          "notice": "Create a new VM in the default namespace from a qcow2 URL.",
          "subtasks": [
            "Create a DataVolume named 'fedora-dv' that imports the Fedora 36 cloud image from 'https://download.fedoraproject.org/pub/fedora/linux/releases/36/Cloud/x86_64/images/Fedora-Cloud-Base-36-1.5.x86_64.qcow2'.",
            "Create a VM named 'vm-from-qcow2' that uses the DataVolume.",
            "Ensure the VM is running.",
            "Verify that you can access the VM's console."
          ]
        }
      ]
    }
  ]
}
      